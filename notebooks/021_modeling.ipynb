{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7a68f9",
   "metadata": {},
   "source": [
    "**Loading Data, see the VIF to find Multicollinearity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2429a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference categories dropped. Ready for VIF check or Modeling.\n",
      "                feature         VIF\n",
      "2              LogPrice  132.674476\n",
      "0              Domestic   89.891337\n",
      "10              To_Rate   43.458594\n",
      "9             From_Rate   42.755284\n",
      "11           Route_Rate   21.274792\n",
      "12            User_Rate    7.730354\n",
      "3           LogLeadTime    4.950620\n",
      "6   TimeOfDay_Afternoon    3.135194\n",
      "7     TimeOfDay_Evening    2.753429\n",
      "1            TripReason    2.699448\n",
      "5         Vehicle_Train    2.430876\n",
      "8     TimeOfDay_Morning    2.203497\n",
      "4         Vehicle_Plane    1.912165\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv')\n",
    "\n",
    "# Convert it to a Series for Statsmodels.\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "\n",
    "# List of reference categories to drop\n",
    "# (We drop 'Bus' and 'Night' so they become the standard baseline)\n",
    "cols_to_drop = ['Vehicle_Bus','TimeOfDay_Night']\n",
    "\n",
    "\n",
    "X_train = X_train.drop(columns=cols_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"Reference categories dropped. Ready for VIF check or Modeling.\")\n",
    "\n",
    "# Create a VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) \n",
    "                   for i in range(len(X_train.columns))]\n",
    "\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0cfd714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               feature       VIF\n",
      "7           Route_Rate  8.017524\n",
      "8            User_Rate  6.427685\n",
      "1          LogLeadTime  4.575417\n",
      "4  TimeOfDay_Afternoon  2.550808\n",
      "3        Vehicle_Train  2.360378\n",
      "0           TripReason  2.304457\n",
      "5    TimeOfDay_Evening  2.297533\n",
      "6    TimeOfDay_Morning  1.866953\n",
      "2        Vehicle_Plane  1.459764\n"
     ]
    }
   ],
   "source": [
    "# 1. Drop the redundant location parts (Keep only Route)\n",
    "cols_to_drop = ['From_Rate', 'To_Rate', 'Domestic']\n",
    "\n",
    "# 2. Drop Price (Because it is highly collinear with Route + Vehicle)\n",
    "cols_to_drop.append('LogPrice')\n",
    "\n",
    "# Apply the drops\n",
    "X_train = X_train.drop(columns=cols_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# 3. Check VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) \n",
    "                   for i in range(len(X_train.columns))]\n",
    "\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce208f81",
   "metadata": {},
   "source": [
    "## Class Imbalance Problem & Solution\n",
    "\n",
    "**Problem:** Our data has 13.7% positive class (cancellations) vs 86.3% negative (no cancellation).\n",
    "- Model learns to predict 0 (non-cancellation) always → High accuracy, but useless!\n",
    "- Precision/Recall become 0 because model never predicts 1.\n",
    "\n",
    "**Solutions:**\n",
    "**Oversampling (SMOTE)** - Create synthetic samples of minority class\n",
    "\n",
    "## Scaler\n",
    "\n",
    "**Problem** High OR in Route_Rate and User_Rate\n",
    "\n",
    "**Solution**\n",
    "We used standardScaler to balanced data to have a data in same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "76d97939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: APPLY SMOTE TO TRAINING DATA ONLY\n",
      "======================================================================\n",
      "\n",
      "BEFORE Oversampling (Training Data):\n",
      "Class 0 (No Cancel): 68553 samples\n",
      "Class 1 (Cancel):    10911 samples\n",
      "Ratio: 13.73% cancellations\n",
      "\n",
      "AFTER Oversampling (Training Data):\n",
      "Class 0 (No Cancel): 68553 samples\n",
      "Class 1 (Cancel):    68553 samples\n",
      "Ratio: 50.00% cancellations\n",
      "\n",
      "======================================================================\n",
      "STEP 2: APPLY STANDARD SCALING\n",
      "======================================================================\n",
      "Scaling applied successfully!\n",
      "X_train_balanced_scaled shape: (137106, 9)\n",
      "X_test_scaled shape: (19866, 9)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: SMOTE + SCALING ===\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: APPLY SMOTE TO TRAINING DATA ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBEFORE Oversampling (Training Data):\")\n",
    "print(f\"Class 0 (No Cancel): {(y_train == 0).sum()} samples\")\n",
    "print(f\"Class 1 (Cancel):    {(y_train == 1).sum()} samples\")\n",
    "print(f\"Ratio: {(y_train == 1).sum() / len(y_train) * 100:.2f}% cancellations\")\n",
    "\n",
    "# Apply SMOTE ONLY to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAFTER Oversampling (Training Data):\")\n",
    "print(f\"Class 0 (No Cancel): {(y_train_balanced == 0).sum()} samples\")\n",
    "print(f\"Class 1 (Cancel):    {(y_train_balanced == 1).sum()} samples\")\n",
    "print(f\"Ratio: {(y_train_balanced == 1).sum() / len(y_train_balanced) * 100:.2f}% cancellations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: APPLY STANDARD SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fit scaler on balanced training data, transform both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train_balanced_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_balanced), \n",
    "    columns=X_train_balanced.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "print(\"Scaling applied successfully!\")\n",
    "print(f\"X_train_balanced_scaled shape: {X_train_balanced_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e1d363d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 3: BASELINE MODEL - LogLeadTime Only\n",
      "======================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 Cancel   No. Observations:               137106\n",
      "Model:                            GLM   Df Residuals:                   137104\n",
      "Model Family:                Binomial   Df Model:                            1\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -92419.\n",
      "Date:                Mon, 26 Jan 2026   Deviance:                   1.8484e+05\n",
      "Time:                        11:18:21   Pearson chi2:                 1.37e+05\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.03743\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const           0.0016      0.006      0.285      0.776      -0.009       0.012\n",
      "LogLeadTime     0.3988      0.006     70.852      0.000       0.388       0.410\n",
      "===============================================================================\n",
      "\n",
      "AIC: 184842.52\n",
      "Log-Likelihood: -92419.26\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: BASELINE MODEL (LogLeadTime Only) ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 3: BASELINE MODEL - LogLeadTime Only\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare baseline data (LogLeadTime only) from scaled data\n",
    "X_train_balanced_scaled_baseline = X_train_balanced_scaled[['LogLeadTime']].copy()\n",
    "X_train_balanced_scaled_baseline_const = sm.add_constant(X_train_balanced_scaled_baseline)\n",
    "\n",
    "X_test_scaled_baseline = X_test_scaled[['LogLeadTime']].copy()\n",
    "X_test_scaled_baseline_const = sm.add_constant(X_test_scaled_baseline)\n",
    "\n",
    "# Fit baseline model\n",
    "baseline_model = sm.GLM(y_train_balanced, X_train_balanced_scaled_baseline_const, \n",
    "                        family=sm.families.Binomial()).fit()\n",
    "\n",
    "print(baseline_model.summary())\n",
    "\n",
    "# Predictions using threshold = 0.5\n",
    "threshold = 0.5\n",
    "y_pred_prob_baseline = baseline_model.predict(X_test_scaled_baseline_const)\n",
    "y_pred_baseline = (y_pred_prob_baseline >= threshold).astype(int)\n",
    "\n",
    "print(f\"\\nAIC: {baseline_model.aic:.2f}\")\n",
    "print(f\"Log-Likelihood: {baseline_model.llf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "78d684a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: FULL MODEL - ALL FEATURES\n",
      "======================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 Cancel   No. Observations:               137106\n",
      "Model:                            GLM   Df Residuals:                   137096\n",
      "Model Family:                Binomial   Df Model:                            9\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -88480.\n",
      "Date:                Mon, 26 Jan 2026   Deviance:                   1.7696e+05\n",
      "Time:                        11:18:21   Pearson chi2:                 1.37e+05\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):            0.09119\n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   0.0101      0.006      1.770      0.077      -0.001       0.021\n",
      "TripReason              0.1299      0.006     20.919      0.000       0.118       0.142\n",
      "LogLeadTime             0.4353      0.007     62.595      0.000       0.422       0.449\n",
      "Vehicle_Plane          -0.1660      0.006    -26.104      0.000      -0.178      -0.154\n",
      "Vehicle_Train          -0.0939      0.007    -13.439      0.000      -0.108      -0.080\n",
      "TimeOfDay_Afternoon    -0.0255      0.008     -3.193      0.001      -0.041      -0.010\n",
      "TimeOfDay_Evening      -0.0552      0.008     -7.090      0.000      -0.070      -0.040\n",
      "TimeOfDay_Morning      -0.0213      0.007     -2.850      0.004      -0.036      -0.007\n",
      "Route_Rate              0.3116      0.006     50.765      0.000       0.300       0.324\n",
      "User_Rate               0.3470      0.006     54.260      0.000       0.334       0.360\n",
      "=======================================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON: BASELINE vs FULL MODEL\n",
      "======================================================================\n",
      "            Metric Baseline (LogLeadTime) Full Model (All)\n",
      "               AIC              184842.52        176979.64\n",
      "               BIC            -1436897.46      -1444681.72\n",
      "    Log-Likelihood              -92419.26        -88479.82\n",
      "Cross-Entropy Loss                 0.6761           0.6464\n",
      "     Features Used                      1                9\n",
      "Metric          Baseline        Full Model     \n",
      "---------------------------------------------\n",
      "Accuracy        0.6016          0.6391         \n",
      "Precision       0.1853          0.2124         \n",
      "Recall          0.5598          0.6012         \n",
      "F1-Score        0.2784          0.3139         \n",
      "ROC-AUC         0.6111          0.6774         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win_10\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1923: FutureWarning: The bic value is computed using the deviance formula. After 0.13 this will change to the log-likelihood based formula. This change has no impact on the relative rank of models compared using BIC. You can directly access the log-likelihood version using the `bic_llf` attribute. You can suppress this message by calling statsmodels.genmod.generalized_linear_model.SET_USE_BIC_LLF with True to get the LLF-based version now or False to retainthe deviance version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: FULL MODEL WITH ALL FEATURES ===\n",
    "from scipy import stats\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: FULL MODEL - ALL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add constant to scaled data\n",
    "X_train_balanced_scaled_const = sm.add_constant(X_train_balanced_scaled)\n",
    "X_test_scaled_const = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Fit full model on balanced and scaled training data\n",
    "full_model = sm.GLM(y_train_balanced, X_train_balanced_scaled_const, \n",
    "                    family=sm.families.Binomial()).fit()\n",
    "\n",
    "print(full_model.summary())\n",
    "\n",
    "# Predictions using threshold = 0.5\n",
    "threshold = 0.5\n",
    "y_pred_prob_full = full_model.predict(X_test_scaled_const)\n",
    "y_pred_full = (y_pred_prob_full >= threshold).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON: BASELINE vs FULL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "baseline_cross_entropy = log_loss(y_test, y_pred_prob_baseline)\n",
    "full_cross_entropy = log_loss(y_test, y_pred_prob_full)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Metric': ['AIC', 'BIC', 'Log-Likelihood', 'Cross-Entropy Loss', 'Features Used'],\n",
    "    'Baseline (LogLeadTime)': [\n",
    "        f\"{baseline_model.aic:.2f}\",\n",
    "        f\"{baseline_model.bic:.2f}\",\n",
    "        f\"{baseline_model.llf:.2f}\",\n",
    "        f\"{baseline_cross_entropy:.4f}\",\n",
    "        \"1\"\n",
    "    ],\n",
    "    'Full Model (All)': [\n",
    "        f\"{full_model.aic:.2f}\",\n",
    "        f\"{full_model.bic:.2f}\",\n",
    "        f\"{full_model.llf:.2f}\",\n",
    "        f\"{full_cross_entropy:.4f}\",\n",
    "        f\"{len(X_train_balanced_scaled.columns)}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"{'Metric':<15} {'Baseline':<15} {'Full Model':<15}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "full_acc = accuracy_score(y_test, y_pred_full)\n",
    "print(f\"{'Accuracy':<15} {baseline_acc:<15.4f} {full_acc:<15.4f}\")\n",
    "\n",
    "baseline_prec = precision_score(y_test, y_pred_baseline, zero_division=0)\n",
    "full_prec = precision_score(y_test, y_pred_full, zero_division=0)\n",
    "print(f\"{'Precision':<15} {baseline_prec:<15.4f} {full_prec:<15.4f}\")\n",
    "\n",
    "baseline_rec = recall_score(y_test, y_pred_baseline, zero_division=0)\n",
    "full_rec = recall_score(y_test, y_pred_full, zero_division=0)\n",
    "print(f\"{'Recall':<15} {baseline_rec:<15.4f} {full_rec:<15.4f}\")\n",
    "\n",
    "baseline_f1 = f1_score(y_test, y_pred_baseline, zero_division=0)\n",
    "full_f1 = f1_score(y_test, y_pred_full, zero_division=0)\n",
    "print(f\"{'F1-Score':<15} {baseline_f1:<15.4f} {full_f1:<15.4f}\")\n",
    "\n",
    "baseline_auc = roc_auc_score(y_test, y_pred_prob_baseline)\n",
    "full_auc = roc_auc_score(y_test, y_pred_prob_full)\n",
    "print(f\"{'ROC-AUC':<15} {baseline_auc:<15.4f} {full_auc:<15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "04477e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 5: FEATURE IMPORTANCE (Odds Ratios) - Scaled Data\n",
      "======================================================================\n",
      "            Feature  Coefficient  Std Error       P-value  Odds Ratio OR Interpretation\n",
      "        LogLeadTime     0.435268   0.006954  0.000000e+00    1.545378  ↑ 54.5% per 1 SD\n",
      "          User_Rate     0.346990   0.006395  0.000000e+00    1.414803  ↑ 41.5% per 1 SD\n",
      "         Route_Rate     0.311553   0.006137  0.000000e+00    1.365544  ↑ 36.6% per 1 SD\n",
      "      Vehicle_Plane    -0.165968   0.006358 3.252737e-150    0.847073  ↓ 15.3% per 1 SD\n",
      "         TripReason     0.129932   0.006211  3.599518e-97    1.138750  ↑ 13.9% per 1 SD\n",
      "      Vehicle_Train    -0.093870   0.006985  3.588965e-41    0.910401   ↓ 9.0% per 1 SD\n",
      "  TimeOfDay_Evening    -0.055191   0.007785  1.343343e-12    0.946304   ↓ 5.4% per 1 SD\n",
      "TimeOfDay_Afternoon    -0.025518   0.007992  1.408579e-03    0.974805   ↓ 2.5% per 1 SD\n",
      "  TimeOfDay_Morning    -0.021293   0.007470  4.367308e-03    0.978932   ↓ 2.1% per 1 SD\n",
      "              const     0.010057   0.005681  7.667994e-02    1.010108   ↑ 1.0% per 1 SD\n",
      "\n",
      "Significant Features (p < 0.05):\n",
      "  ✓ LogLeadTime                    → INCREASES  (p=0.0000, OR=1.545)\n",
      "  ✓ User_Rate                      → INCREASES  (p=0.0000, OR=1.415)\n",
      "  ✓ Route_Rate                     → INCREASES  (p=0.0000, OR=1.366)\n",
      "  ✓ Vehicle_Plane                  → DECREASES  (p=0.0000, OR=0.847)\n",
      "  ✓ TripReason                     → INCREASES  (p=0.0000, OR=1.139)\n",
      "  ✓ Vehicle_Train                  → DECREASES  (p=0.0000, OR=0.910)\n",
      "  ✓ TimeOfDay_Evening              → DECREASES  (p=0.0000, OR=0.946)\n",
      "  ✓ TimeOfDay_Afternoon            → DECREASES  (p=0.0014, OR=0.975)\n",
      "  ✓ TimeOfDay_Morning              → DECREASES  (p=0.0044, OR=0.979)\n",
      "\n",
      "Insignificant Features (p ≥ 0.05):\n",
      "  ✗ const                          (p=0.0767)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: FEATURE IMPORTANCE & ODDS RATIOS ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 5: FEATURE IMPORTANCE (Odds Ratios) - Scaled Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Extract coefficients and calculate odds ratios\n",
    "coef_summary = pd.DataFrame({\n",
    "    'Feature': X_train_balanced_scaled_const.columns,\n",
    "    'Coefficient': full_model.params.values,\n",
    "    'Std Error': full_model.bse.values,\n",
    "    'P-value': full_model.pvalues.values\n",
    "})\n",
    "\n",
    "# Calculate Odds Ratio = exp(coefficient)\n",
    "coef_summary['Odds Ratio'] = np.exp(coef_summary['Coefficient'])\n",
    "coef_summary['OR Interpretation'] = coef_summary['Odds Ratio'].apply(\n",
    "    lambda x: f\"{'↑' if x > 1 else '↓'} {abs((x-1)*100):.1f}% per 1 SD\" if x != 1 else \"No effect\"\n",
    ")\n",
    "\n",
    "# Sort by absolute coefficient value\n",
    "coef_summary['Abs_Coef'] = abs(coef_summary['Coefficient'])\n",
    "coef_summary_sorted = coef_summary.sort_values('Abs_Coef', ascending=False).drop('Abs_Coef', axis=1)\n",
    "\n",
    "print(coef_summary_sorted.to_string(index=False))\n",
    "\n",
    "# Show significant features\n",
    "print(\"\\nSignificant Features (p < 0.05):\")\n",
    "significant = coef_summary_sorted[coef_summary_sorted['P-value'] < 0.05]\n",
    "for idx, row in significant.iterrows(): \n",
    "    effect = \"INCREASES\" if row['Coefficient'] > 0 else \"DECREASES\"\n",
    "    print(f\"  ✓ {row['Feature']:30} → {effect:10} (p={row['P-value']:.4f}, OR={row['Odds Ratio']:.3f})\")\n",
    "\n",
    "print(\"\\nInsignificant Features (p ≥ 0.05):\")\n",
    "insignificant = coef_summary_sorted[coef_summary_sorted['P-value'] >= 0.05]\n",
    "for idx, row in insignificant.iterrows():  \n",
    "    print(f\"  ✗ {row['Feature']:30} (p={row['P-value']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
