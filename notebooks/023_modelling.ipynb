{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c59f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19367ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9aae413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete. Training: 79464 rows, Test: 19866 rows.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79464 entries, 0 to 79463\n",
      "Data columns (total 17 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Domestic                           79464 non-null  int64  \n",
      " 1   TripReason                         79464 non-null  int64  \n",
      " 2   Cancel                             79464 non-null  int64  \n",
      " 3   LeadTime_Days                      79464 non-null  float64\n",
      " 4   LogPrice                           79464 non-null  float64\n",
      " 5   Vehicle_Bus                        79464 non-null  int64  \n",
      " 6   Vehicle_Plane                      79464 non-null  int64  \n",
      " 7   Vehicle_Train                      79464 non-null  int64  \n",
      " 8   TimeOfDay_Afternoon                79464 non-null  int64  \n",
      " 9   TimeOfDay_Evening                  79464 non-null  int64  \n",
      " 10  TimeOfDay_Morning                  79464 non-null  int64  \n",
      " 11  TimeOfDay_Night                    79464 non-null  int64  \n",
      " 12  From_Rate                          79464 non-null  float64\n",
      " 13  To_Rate                            79464 non-null  float64\n",
      " 14  Route_Rate                         79464 non-null  float64\n",
      " 15  User_Rate                          79464 non-null  float64\n",
      " 16  cancel_rate_per_vehicle_and_price  79464 non-null  float64\n",
      "dtypes: float64(7), int64(10)\n",
      "memory usage: 10.3 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domestic</th>\n",
       "      <th>TripReason</th>\n",
       "      <th>Cancel</th>\n",
       "      <th>LeadTime_Days</th>\n",
       "      <th>LogPrice</th>\n",
       "      <th>Vehicle_Bus</th>\n",
       "      <th>Vehicle_Plane</th>\n",
       "      <th>Vehicle_Train</th>\n",
       "      <th>TimeOfDay_Afternoon</th>\n",
       "      <th>TimeOfDay_Evening</th>\n",
       "      <th>TimeOfDay_Morning</th>\n",
       "      <th>TimeOfDay_Night</th>\n",
       "      <th>From_Rate</th>\n",
       "      <th>To_Rate</th>\n",
       "      <th>Route_Rate</th>\n",
       "      <th>User_Rate</th>\n",
       "      <th>cancel_rate_per_vehicle_and_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.597348</td>\n",
       "      <td>15.702580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145227</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.156824</td>\n",
       "      <td>0.114425</td>\n",
       "      <td>0.113812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.557834</td>\n",
       "      <td>16.066802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163560</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.182062</td>\n",
       "      <td>0.137310</td>\n",
       "      <td>0.113812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732323</td>\n",
       "      <td>14.508658</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.104223</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.145818</td>\n",
       "      <td>0.114425</td>\n",
       "      <td>0.125352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.872986</td>\n",
       "      <td>10.596635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127785</td>\n",
       "      <td>0.031021</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>0.114425</td>\n",
       "      <td>0.117717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.478546</td>\n",
       "      <td>13.937728</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142249</td>\n",
       "      <td>0.115205</td>\n",
       "      <td>0.093713</td>\n",
       "      <td>0.098079</td>\n",
       "      <td>0.125350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Domestic  TripReason  Cancel  LeadTime_Days   LogPrice  Vehicle_Bus  \\\n",
       "0         1           1       0      10.597348  15.702580            0   \n",
       "1         1           0       0       2.557834  16.066802            0   \n",
       "2         1           1       0       0.732323  14.508658            1   \n",
       "3         1           0       0       0.872986  10.596635            0   \n",
       "4         1           0       0       1.478546  13.937728            1   \n",
       "\n",
       "   Vehicle_Plane  Vehicle_Train  TimeOfDay_Afternoon  TimeOfDay_Evening  \\\n",
       "0              1              0                    0                  0   \n",
       "1              1              0                    0                  0   \n",
       "2              0              0                    0                  0   \n",
       "3              0              1                    0                  0   \n",
       "4              0              0                    0                  0   \n",
       "\n",
       "   TimeOfDay_Morning  TimeOfDay_Night  From_Rate   To_Rate  Route_Rate  \\\n",
       "0                  0                1   0.145227  0.148492    0.156824   \n",
       "1                  0                1   0.163560  0.148492    0.182062   \n",
       "2                  1                0   0.104223  0.185071    0.145818   \n",
       "3                  1                0   0.127785  0.031021    0.035157   \n",
       "4                  0                1   0.142249  0.115205    0.093713   \n",
       "\n",
       "   User_Rate  cancel_rate_per_vehicle_and_price  \n",
       "0   0.114425                           0.113812  \n",
       "1   0.137310                           0.113812  \n",
       "2   0.114425                           0.125352  \n",
       "3   0.114425                           0.117717  \n",
       "4   0.098079                           0.125350  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/updated_data2.csv\")\n",
    "\n",
    " # Split the data\n",
    "id_col=\"Index\", \n",
    "target_col='Cancel', \n",
    "train_size=0.8\n",
    "split_index = int(len(df) * train_size)\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df  = df.iloc[split_index:].copy()\n",
    "    \n",
    "print(f\"Split complete. Training: {len(train_df)} rows, Test: {len(test_df)} rows.\")\n",
    "\n",
    "# Final Clean-up: drop the ID and the Date column\n",
    "train_df = train_df.drop(columns=[id_col[0]])\n",
    "test_df = test_df.drop(columns=[id_col[0]])\n",
    "\n",
    "X_train = train_df.drop(columns=['Cancel'])\n",
    "y_train = train_df['Cancel']\n",
    "\n",
    "X_test = test_df.drop(columns=['Cancel'])\n",
    "y_test = test_df['Cancel']\n",
    "\n",
    "# Convert it to a Series for Statsmodels.\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print(train_df.info())\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b67454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference categories dropped. Ready for VIF check or Modeling.\n",
      "               feature       VIF\n",
      "7           Route_Rate  7.405538\n",
      "8            User_Rate  5.769223\n",
      "4  TimeOfDay_Afternoon  2.473931\n",
      "0           TripReason  2.346586\n",
      "5    TimeOfDay_Evening  2.248752\n",
      "3        Vehicle_Train  2.185912\n",
      "1        LeadTime_Days  2.003988\n",
      "6    TimeOfDay_Morning  1.832509\n",
      "2        Vehicle_Plane  1.373849\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of reference categories to drop\n",
    "# (We drop 'Bus' and 'Night' so they become the standard baseline)\n",
    "\n",
    "cols_to_drop = ['Vehicle_Bus','TimeOfDay_Night','From_Rate', 'To_Rate', 'Domestic','LogPrice','cancel_rate_per_vehicle_and_price']\n",
    "\n",
    "# Apply the drops\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "print(\"Reference categories dropped. Ready for VIF check or Modeling.\")\n",
    "\n",
    "# Create a VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) \n",
    "                   for i in range(len(X_train.columns))]\n",
    "\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f89c6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: APPLY SMOTE TO TRAINING DATA ONLY\n",
      "======================================================================\n",
      "\n",
      "BEFORE Oversampling (Training Data):\n",
      "Class 0 (No Cancel): 68600 samples\n",
      "Class 1 (Cancel):    10864 samples\n",
      "Ratio: 13.67% cancellations\n",
      "\n",
      "AFTER Oversampling (Training Data):\n",
      "Class 0 (No Cancel): 68600 samples\n",
      "Class 1 (Cancel):    68600 samples\n",
      "Ratio: 50.00% cancellations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: APPLY SMOTE TO TRAINING DATA ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBEFORE Oversampling (Training Data):\")\n",
    "print(f\"Class 0 (No Cancel): {(y_train == 0).sum()} samples\")\n",
    "print(f\"Class 1 (Cancel):    {(y_train == 1).sum()} samples\")\n",
    "print(f\"Ratio: {((y_train == 1).sum() / len(y_train)) * 100:.2f}% cancellations\")\n",
    "\n",
    "# Apply SMOTE ONLY to training data\n",
    "# SMOTE creates synthetic samples of the minority class (1) to match majority class (0)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"\\nAFTER Oversampling (Training Data):\")\n",
    "print(f\"Class 0 (No Cancel): {(y_train_balanced == 0).sum()} samples\")\n",
    "print(f\"Class 1 (Cancel):    {(y_train_balanced == 1).sum()} samples\")\n",
    "print(f\"Ratio: {((y_train_balanced == 1).sum() / len(y_train_balanced)) * 100:.2f}% cancellations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7af4b2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: APPLY STANDARD SCALING\n",
      "======================================================================\n",
      "Scaling applied successfully!\n",
      "X_train_balanced_scaled shape: (137200, 9)\n",
      "X_test_scaled shape: (19866, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: APPLY STANDARD SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fit scaler on balanced training data, transform both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train_balanced_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_balanced), \n",
    "    columns=X_train_balanced.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "print(\"Scaling applied successfully!\")\n",
    "print(f\"X_train_balanced_scaled shape: {X_train_balanced_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "726375e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 3: BASELINE MODEL - cancel_rate_per_vehicle_and_price Only\n",
      "======================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 Cancel   No. Observations:               137200\n",
      "Model:                            GLM   Df Residuals:                   137198\n",
      "Model Family:                Binomial   Df Model:                            1\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -92038.\n",
      "Date:                Tue, 27 Jan 2026   Deviance:                   1.8408e+05\n",
      "Time:                        15:10:58   Pearson chi2:                 1.38e+05\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.04366\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0135      0.006      2.445      0.014       0.003       0.024\n",
      "User_Rate      0.4613      0.006     72.346      0.000       0.449       0.474\n",
      "==============================================================================\n",
      "\n",
      "AIC: 184079.20\n",
      "Log-Likelihood: -92037.60\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: BASELINE MODEL (LogLeadTime Only) ===\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 3: BASELINE MODEL - cancel_rate_per_vehicle_and_price Only\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare baseline data (LogLeadTime only) from scaled data\n",
    "X_train_balanced_scaled_baseline = X_train_balanced_scaled[['User_Rate']].copy()\n",
    "X_train_balanced_scaled_baseline_const = sm.add_constant(X_train_balanced_scaled_baseline)\n",
    "\n",
    "X_test_scaled_baseline = X_test_scaled[['User_Rate']].copy()\n",
    "X_test_scaled_baseline_const = sm.add_constant(X_test_scaled_baseline)\n",
    "\n",
    "# Fit baseline model\n",
    "baseline_model = sm.GLM(y_train_balanced, X_train_balanced_scaled_baseline_const, \n",
    "                        family=sm.families.Binomial()).fit()\n",
    "\n",
    "print(baseline_model.summary())\n",
    "\n",
    "# Predictions using threshold = 0.5\n",
    "threshold = 0.5\n",
    "y_pred_prob_baseline = baseline_model.predict(X_test_scaled_baseline_const)\n",
    "y_pred_baseline = (y_pred_prob_baseline >= threshold).astype(int)\n",
    "\n",
    "print(f\"\\nAIC: {baseline_model.aic:.2f}\")\n",
    "print(f\"Log-Likelihood: {baseline_model.llf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "057af2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: FULL MODEL - ALL FEATURES\n",
      "======================================================================\n",
      "Features nel modello (training): ['const', 'TripReason', 'LeadTime_Days', 'Vehicle_Plane', 'Vehicle_Train', 'TimeOfDay_Afternoon', 'TimeOfDay_Evening', 'TimeOfDay_Morning', 'Route_Rate', 'User_Rate']\n",
      "Features nel dataset di test: ['const', 'TripReason', 'LeadTime_Days', 'Vehicle_Plane', 'Vehicle_Train', 'TimeOfDay_Afternoon', 'TimeOfDay_Evening', 'TimeOfDay_Morning', 'Route_Rate', 'User_Rate']\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 Cancel   No. Observations:               137200\n",
      "Model:                            GLM   Df Residuals:                   137190\n",
      "Model Family:                Binomial   Df Model:                            9\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -89266.\n",
      "Date:                Tue, 27 Jan 2026   Deviance:                   1.7853e+05\n",
      "Time:                        15:10:59   Pearson chi2:                 1.40e+05\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.08152\n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   0.0145      0.006      2.567      0.010       0.003       0.026\n",
      "TripReason              0.0910      0.006     14.737      0.000       0.079       0.103\n",
      "LeadTime_Days           0.2531      0.007     36.373      0.000       0.239       0.267\n",
      "Vehicle_Plane          -0.1071      0.006    -17.409      0.000      -0.119      -0.095\n",
      "Vehicle_Train          -0.0143      0.007     -2.119      0.034      -0.028      -0.001\n",
      "TimeOfDay_Afternoon    -0.0327      0.008     -4.140      0.000      -0.048      -0.017\n",
      "TimeOfDay_Evening      -0.0642      0.008     -8.339      0.000      -0.079      -0.049\n",
      "TimeOfDay_Morning      -0.0345      0.007     -4.673      0.000      -0.049      -0.020\n",
      "Route_Rate              0.3267      0.006     52.733      0.000       0.315       0.339\n",
      "User_Rate               0.4007      0.006     61.988      0.000       0.388       0.413\n",
      "=======================================================================================\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON: BASELINE vs FULL MODEL\n",
      "======================================================================\n",
      "            Metric Baseline (LogLeadTime) Full Model (All)\n",
      "               AIC              184079.20        178552.22\n",
      "               BIC            -1438866.69      -1444315.04\n",
      "    Log-Likelihood              -92037.60        -89266.11\n",
      "Cross-Entropy Loss                 0.6721           0.6518\n",
      "     Features Used                      1                9\n",
      "Metric          Baseline        Full Model     \n",
      "---------------------------------------------\n",
      "Accuracy        0.7970          0.6586         \n",
      "Precision       0.2900          0.2159         \n",
      "Recall          0.3128          0.5488         \n",
      "F1-Score        0.3010          0.3099         \n",
      "ROC-AUC         0.5957          0.6628         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessiovalle/GitRepos/statistical-methods-project/venv/lib/python3.12/site-packages/statsmodels/genmod/generalized_linear_model.py:1923: FutureWarning: The bic value is computed using the deviance formula. After 0.13 this will change to the log-likelihood based formula. This change has no impact on the relative rank of models compared using BIC. You can directly access the log-likelihood version using the `bic_llf` attribute. You can suppress this message by calling statsmodels.genmod.generalized_linear_model.SET_USE_BIC_LLF with True to get the LLF-based version now or False to retainthe deviance version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: FULL MODEL WITH ALL FEATURES ===\n",
    "from scipy import stats\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: FULL MODEL - ALL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add constant to scaled data\n",
    "X_train_balanced_scaled_const = sm.add_constant(X_train_balanced_scaled)\n",
    "X_test_scaled_const = sm.add_constant(X_test_scaled, has_constant='add')\n",
    "\n",
    "# Per evitare altri ValueError, assicurati che l'ordine delle colonne sia IDENTICO al train\n",
    "X_test_scaled_const = X_test_scaled_const[X_train_balanced_scaled_const.columns]\n",
    "\n",
    "# Fit full model on balanced and scaled training data\n",
    "full_model = sm.GLM(y_train_balanced, X_train_balanced_scaled_const, \n",
    "                    family=sm.families.Binomial()).fit()\n",
    "\n",
    "print(f\"Features nel modello (training): {full_model.params.index.tolist()}\")\n",
    "print(f\"Features nel dataset di test: {X_test_scaled_const.columns.tolist()}\")\n",
    "print(full_model.summary())\n",
    "\n",
    "# Predictions using threshold = 0.5\n",
    "threshold = 0.5\n",
    "y_pred_prob_full = full_model.predict(X_test_scaled_const)\n",
    "y_pred_full = (y_pred_prob_full >= threshold).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON: BASELINE vs FULL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "baseline_cross_entropy = log_loss(y_test, y_pred_prob_baseline)\n",
    "full_cross_entropy = log_loss(y_test, y_pred_prob_full)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Metric': ['AIC', 'BIC', 'Log-Likelihood', 'Cross-Entropy Loss', 'Features Used'],\n",
    "    'Baseline (LogLeadTime)': [\n",
    "        f\"{baseline_model.aic:.2f}\",\n",
    "        f\"{baseline_model.bic:.2f}\",\n",
    "        f\"{baseline_model.llf:.2f}\",\n",
    "        f\"{baseline_cross_entropy:.4f}\",\n",
    "        \"1\"\n",
    "    ],\n",
    "    'Full Model (All)': [\n",
    "        f\"{full_model.aic:.2f}\",\n",
    "        f\"{full_model.bic:.2f}\",\n",
    "        f\"{full_model.llf:.2f}\",\n",
    "        f\"{full_cross_entropy:.4f}\",\n",
    "        f\"{len(X_train_balanced_scaled.columns)}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"{'Metric':<15} {'Baseline':<15} {'Full Model':<15}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "full_acc = accuracy_score(y_test, y_pred_full)\n",
    "print(f\"{'Accuracy':<15} {baseline_acc:<15.4f} {full_acc:<15.4f}\")\n",
    "\n",
    "baseline_prec = precision_score(y_test, y_pred_baseline, zero_division=0)\n",
    "full_prec = precision_score(y_test, y_pred_full, zero_division=0)\n",
    "print(f\"{'Precision':<15} {baseline_prec:<15.4f} {full_prec:<15.4f}\")\n",
    "\n",
    "baseline_rec = recall_score(y_test, y_pred_baseline, zero_division=0)\n",
    "full_rec = recall_score(y_test, y_pred_full, zero_division=0)\n",
    "print(f\"{'Recall':<15} {baseline_rec:<15.4f} {full_rec:<15.4f}\")\n",
    "\n",
    "baseline_f1 = f1_score(y_test, y_pred_baseline, zero_division=0)\n",
    "full_f1 = f1_score(y_test, y_pred_full, zero_division=0)\n",
    "print(f\"{'F1-Score':<15} {baseline_f1:<15.4f} {full_f1:<15.4f}\")\n",
    "\n",
    "baseline_auc = roc_auc_score(y_test, y_pred_prob_baseline)\n",
    "full_auc = roc_auc_score(y_test, y_pred_prob_full)\n",
    "print(f\"{'ROC-AUC':<15} {baseline_auc:<15.4f} {full_auc:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9358041",
   "metadata": {},
   "source": [
    "test of mixture of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8964229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: BASEmy\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 4: BASEmy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def mixture_model_create_and_compare(X_train_balanced_scaled,X_test_scaled,y_train_balanced, y_test, y_pred_prob_baseline,baseline_model, y_pred_baseline, columns):\n",
    "    # Add constant to scaled data\n",
    "    X_train_balanced_scaled_const = sm.add_constant(X_train_balanced_scaled)\n",
    "    X_test_scaled_const = sm.add_constant(X_test_scaled, has_constant='add')\n",
    "\n",
    "    # Per evitare altri ValueError, assicurati che l'ordine delle colonne sia IDENTICO al train\n",
    "    X_test_scaled_const = X_test_scaled_const[X_train_balanced_scaled_const.columns]\n",
    "\n",
    "    # Fit full model on balanced and scaled training data\n",
    "    full_model = sm.GLM(y_train_balanced, X_train_balanced_scaled_const, \n",
    "                        family=sm.families.Binomial()).fit()\n",
    "\n",
    "    print(f\"Features nel modello (training): {full_model.params.index.tolist()}\")\n",
    "    print(f\"Features nel dataset di test: {X_test_scaled_const.columns.tolist()}\")\n",
    "    print(full_model.summary())\n",
    "\n",
    "    # Predictions using threshold = 0.5\n",
    "    threshold = 0.5\n",
    "    y_pred_prob_full = full_model.predict(X_test_scaled_const)\n",
    "    y_pred_full = (y_pred_prob_full >= threshold).astype(int)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON: BASELINE vs FULL MODEL\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Cross-Entropy Loss\n",
    "    baseline_cross_entropy = log_loss(y_test, y_pred_prob_baseline)\n",
    "    full_cross_entropy = log_loss(y_test, y_pred_prob_full)\n",
    "\n",
    "    # Create comparison dataframe\n",
    "    comparison_data = {\n",
    "        'Metric': ['AIC', 'BIC', 'Log-Likelihood', 'Cross-Entropy Loss', 'Features Used'],\n",
    "        'Baseline (LogLeadTime)': [\n",
    "            f\"{baseline_model.aic:.2f}\",\n",
    "            f\"{baseline_model.bic:.2f}\",\n",
    "            f\"{baseline_model.llf:.2f}\",\n",
    "            f\"{baseline_cross_entropy:.4f}\",\n",
    "            \"1\"\n",
    "        ],\n",
    "        'Full Model (All)': [\n",
    "            f\"{full_model.aic:.2f}\",\n",
    "            f\"{full_model.bic:.2f}\",\n",
    "            f\"{full_model.llf:.2f}\",\n",
    "            f\"{full_cross_entropy:.4f}\",\n",
    "            f\"{len(X_train_balanced_scaled.columns)}\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "\n",
    "    # Performance comparison\n",
    "    print(f\"{'Metric':<15} {'Baseline':<15} {'Full Model':<15}\")\n",
    "    print(\"-\"*45)\n",
    "\n",
    "    baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "    full_acc = accuracy_score(y_test, y_pred_full)\n",
    "    print(f\"{'Accuracy':<15} {baseline_acc:<15.4f} {full_acc:<15.4f}\")\n",
    "\n",
    "    baseline_prec = precision_score(y_test, y_pred_baseline, zero_division=0)\n",
    "    full_prec = precision_score(y_test, y_pred_full, zero_division=0)\n",
    "    print(f\"{'Precision':<15} {baseline_prec:<15.4f} {full_prec:<15.4f}\")\n",
    "\n",
    "    baseline_rec = recall_score(y_test, y_pred_baseline, zero_division=0)\n",
    "    full_rec = recall_score(y_test, y_pred_full, zero_division=0)\n",
    "    print(f\"{'Recall':<15} {baseline_rec:<15.4f} {full_rec:<15.4f}\")\n",
    "\n",
    "    baseline_f1 = f1_score(y_test, y_pred_baseline, zero_division=0)\n",
    "    full_f1 = f1_score(y_test, y_pred_full, zero_division=0)\n",
    "    print(f\"{'F1-Score':<15} {baseline_f1:<15.4f} {full_f1:<15.4f}\")\n",
    "\n",
    "    baseline_auc = roc_auc_score(y_test, y_pred_prob_baseline)\n",
    "    full_auc = roc_auc_score(y_test, y_pred_prob_full)\n",
    "    print(f\"{'ROC-AUC':<15} {baseline_auc:<15.4f} {full_auc:<15.4f}\")\n",
    "    return full_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
